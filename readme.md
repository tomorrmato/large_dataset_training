Training a model with a large dataset can lead to out of memory (OOM) issues, which occurs when the model requires more memory than is available. It's very easy to implement customized pytorch dataloader, but for certain  use cases, for example, When the dataset is very large, it's important to design a dataloader that loads necessary batch in memory.

In certain NLP use cases, when the token ids are native python objects of variational length, such as type `List[int]`, native python objects may cause memory leaking issues in iterating each epoch (check out issue https://github.com/pytorch/pytorch/issues/13246). Using memory efficient data like numpy array, tfrecord will resolve the memory leaking issue. 

In this repo, `config.py` is the data config, `write_data.py` is the script to generate data for running `large_dataset_training.py`. `large_dataset_training.py` has two parts: 1. `DALI pipeline` and 2. `pytorch lightning module with DALI dataloader`, the key in part 2 is the `train_dataloader`, in distributed data parallel (DDP) training, it's necessary to shard training data for each device, check out the implementation for more details. DALI pipeline has been tested to load TB level of data with pretty impressive performance! 
